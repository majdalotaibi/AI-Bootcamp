{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Kkac5MYVx0"
      },
      "source": [
        "# Introduction\n",
        "In this tutorial, we will explore how to use various bagging algorithms to predict income levels using the Adult dataset. The Adult dataset, also known as the \"Census Income\" dataset, contains data on individuals from the 1994 Census database. The task is to predict whether a person's income exceeds $50,000/year based on their demographic attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjzN1q_TZ0Y0"
      },
      "source": [
        "# Steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3io1-2AZ3Iq"
      },
      "source": [
        "## Step 1: Import Required Libraries\n",
        "First, import the necessary libraries for data manipulation, model training, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QPI1xPaWYVIA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-yVfY8jaocp"
      },
      "source": [
        "## Step 2: Load and Preprocess the Dataset\n",
        "Load the Adult dataset and preprocess it. This includes handling missing values, encoding categorical variables, and splitting the data into features and target variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lcORhlKQYSm-"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation',\n",
        "           'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']\n",
        "data = pd.read_csv(url, header=None, names=columns, na_values=' ?')\n",
        "\n",
        "# Handle missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Encode categorical variables\n",
        "data = pd.get_dummies(data, drop_first=True)\n",
        "\n",
        "# Split the data into features and target variable\n",
        "X = data.drop('income_ >50K', axis=1)\n",
        "y = data['income_ >50K']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fLTji5Oa1dG"
      },
      "source": [
        "## Step 3: Split the Dataset\n",
        "Split the dataset into training and testing sets to evaluate the performance of the models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hTjFxaRVa1Gh"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODx6_33fbHVB"
      },
      "source": [
        "## Step 4: Initialize and Train the Classifiers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWCxRi7KbQfs"
      },
      "source": [
        "### Bagging Meta-estimator\n",
        "Initialize a K-Nearest Neighbors classifier and use it as the base estimator for the Bagging classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xMMX9AwbTZN",
        "outputId": "354a04ad-5f5e-4fcf-8941-2ecfdf18a900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bagging Classifier Model Accuracy: 76.84%\n"
          ]
        }
      ],
      "source": [
        "# Initialize base classifier and Bagging Meta-estimator\n",
        "base_estimator = KNeighborsClassifier()\n",
        "bagging_classifier = BaggingClassifier(base_estimator, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f'Bagging Classifier Model Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhdzDdeJbYWl"
      },
      "source": [
        "### Random Forest\n",
        "Initialize and train a Random Forest classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcTbXnSHbghT"
      },
      "outputs": [],
      "source": [
        "# Initialize and train the Random Forest classifier\n",
        "random_forest_classifier = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "random_forest_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = random_forest_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f'Random Forest Model Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An9qz2T3bjoa"
      },
      "source": [
        "### Pasting\n",
        "Initialize a Decision Tree classifier and use it as the base estimator for a Bagging classifier with Pasting (without replacement)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2HGiN7UbpU1"
      },
      "outputs": [],
      "source": [
        "# Initialize the base classifier and Bagging Meta-estimator with Pasting\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "pasting_classifier = BaggingClassifier(base_estimator, n_estimators=50, max_samples=0.7, bootstrap=False, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "pasting_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = pasting_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f'Pasting Classifier Model Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgyfK2H8bu60"
      },
      "source": [
        "### Roughly Balanced Bagging (RBB)\n",
        "Implement Roughly Balanced Bagging by manually creating balanced bootstrap samples and aggregating predictions from multiple Decision Tree classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZhUecqEbxjn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Number of base estimators\n",
        "n_estimators = 100\n",
        "\n",
        "# Initialize arrays to store the ensemble predictions and models\n",
        "ensemble_preds = np.zeros((n_estimators, len(X_test)))\n",
        "ensemble_models = []\n",
        "\n",
        "for i in range(n_estimators):\n",
        "    # Create a bootstrap sample, ensuring it's roughly balanced\n",
        "    pos_indices = np.where(y_train == 1)[0]\n",
        "    neg_indices = np.where(y_train == 0)[0]\n",
        "\n",
        "    chosen_pos_indices = np.random.choice(pos_indices, size=len(pos_indices), replace=True)\n",
        "    chosen_neg_indices = np.random.choice(neg_indices, size=len(pos_indices), replace=True)\n",
        "\n",
        "    balanced_sample_indices = np.concatenate([chosen_pos_indices, chosen_neg_indices])\n",
        "    np.random.shuffle(balanced_sample_indices)\n",
        "\n",
        "    X_train_balanced = X_train.iloc[balanced_sample_indices]\n",
        "    y_train_balanced = y_train.iloc[balanced_sample_indices]\n",
        "\n",
        "    # Train a decision tree classifier on the balanced bootstrap sample\n",
        "    tree_clf = DecisionTreeClassifier(random_state=i)\n",
        "    tree_clf.fit(X_train_balanced, y_train_balanced)\n",
        "    ensemble_models.append(tree_clf)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    ensemble_preds[i] = tree_clf.predict(X_test)\n",
        "\n",
        "# Majority voting across all estimators for the final prediction\n",
        "final_preds = np.round(np.mean(ensemble_preds, axis=0))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, final_preds)\n",
        "print(f'Roughly Balanced Bagging Model Accuracy: {accuracy:.2f}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
