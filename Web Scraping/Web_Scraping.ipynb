{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e18d10",
   "metadata": {},
   "source": [
    "# Web Scraping Tutorial\n",
    "\n",
    "This tutorial will cover the basics of web scraping using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd24ac",
   "metadata": {},
   "source": [
    "## What is Web Scraping?\n",
    "\n",
    "Web scraping is the process of automatically extracting data from websites. It can be done using various tools and libraries in Python, such as BeautifulSoup and Scrapy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089292c0",
   "metadata": {},
   "source": [
    "## Installing Required Libraries\n",
    "\n",
    "For this tutorial, we will use `requests` to fetch web pages and `BeautifulSoup` to parse HTML content. Install these libraries using pip:\n",
    "```bash\n",
    "pip install requests\n",
    "pip install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed76d0ca",
   "metadata": {},
   "source": [
    "## Fetching a Web Page\n",
    "\n",
    "Use the `requests` library to fetch the content of a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd9a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the web page you want to scrape\n",
    "url = 'https://en.wikipedia.org/wiki/Artificial_intelligence'\n",
    "\n",
    "# Fetch the content of the web page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Print the status code to check if the request was successful\n",
    "print(response.status_code)\n",
    "\n",
    "# Print the first 500 characters of the content\n",
    "print(response.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51598ad",
   "metadata": {},
   "source": [
    "## Parsing HTML Content\n",
    "\n",
    "Use `BeautifulSoup` to parse the HTML content of the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d0d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Create a BeautifulSoup object and specify the parser\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Print the title of the web page\n",
    "print(soup.title)\n",
    "\n",
    "# Print all paragraph tags\n",
    "for p in soup.find_all('p'):\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f843dd4",
   "metadata": {},
   "source": [
    "## Extracting Specific Data\n",
    "\n",
    "Extract specific data from the web page using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4822af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all links from the web page\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# Print the URLs of the links\n",
    "for link in links:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e62091",
   "metadata": {},
   "source": [
    "## Saving Data to a File\n",
    "\n",
    "Save the extracted data to a file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('links.txt', 'w') as file:\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href is not None:\n",
    "            file.write(href + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f09d4c3",
   "metadata": {},
   "source": [
    "## Handling Pagination\n",
    "\n",
    "Some websites have multiple pages of data. Handle pagination by iterating through each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbae660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the first 5 pages\n",
    "for page in range(1, 6):\n",
    "    url = f'https://en.wikipedia.org/wiki/{page}'  # Update this URL to the actual website you are scraping\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all titles and paragraphs\n",
    "    titles = soup.find_all('h1')  # Assuming titles are in <h1> tags\n",
    "    paragraphs = soup.find_all('p')\n",
    "    \n",
    "    # Print titles\n",
    "    print(f\"Page {page} Titles:\")\n",
    "    for title in titles:\n",
    "        print(title.get_text())\n",
    "        \n",
    "    print(f\"\\nPage {page} Paragraphs:\")\n",
    "    for paragraph in paragraphs:\n",
    "        print(paragraph.get_text())\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a55704",
   "metadata": {},
   "source": [
    "## Extracting Data from Tables\n",
    "\n",
    "Web pages often contain data in HTML tables. You can extract this data using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from a table\n",
    "table = soup.find('table')\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    for col in cols:\n",
    "        print(col.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a3d1c",
   "metadata": {},
   "source": [
    "## Handling JavaScript-Rendered Content\n",
    "\n",
    "Some web pages render content using JavaScript, which requires different approaches to scrape, such as using Selenium or Scrapy with Splash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39091c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using Selenium to handle JavaScript-rendered content\n",
    "# Install Selenium using pip: pip install selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "# Set up the WebDriver (e.g., using Chrome)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the web page\n",
    "driver.get('http://example.com')\n",
    "\n",
    "# Get the page source and parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Extract data as usual\n",
    "print(soup.title.text)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c42a19",
   "metadata": {},
   "source": [
    "## Using Scrapy for Advanced Web Scraping\n",
    "\n",
    "Scrapy is a powerful web scraping framework for Python. It allows you to define spiders to crawl and scrape websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48305a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a simple Scrapy spider\n",
    "# Save this code in a file named my_spider.py\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'my_spider'\n",
    "    start_urls = ['http://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').get()\n",
    "        yield {'title': title}\n",
    "\n",
    "# Run the spider using the command: scrapy runspider my_spider.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4065c",
   "metadata": {},
   "source": [
    "## Avoiding Getting Blocked\n",
    "\n",
    "When scraping websites, itâ€™s important to follow best practices to avoid getting blocked. Here are some tips:\n",
    "\n",
    "1. **Respect `robots.txt`:** Check the website's `robots.txt` file to see which parts of the site are allowed to be scraped.\n",
    "2. **Throttle your requests:** Do not overload the server with too many requests in a short period. Use delays and random intervals between requests.\n",
    "3. **Use User-Agent headers:** Some websites block requests that do not come from a browser. Set the User-Agent header to mimic a real browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732a034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of setting headers to mimic a real browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a34ea",
   "metadata": {},
   "source": [
    "## Error Handling\n",
    "\n",
    "Implement error handling to make your scraper more robust and handle exceptions gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aafab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of error handling\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "except requests.exceptions.HTTPError as err:\n",
    "    print(f'HTTP error occurred: {err}')\n",
    "except Exception as err:\n",
    "    print(f'Other error occurred: {err}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22192fbb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we've covered the basics of web scraping using Python, including fetching web pages, parsing HTML content, extracting specific data, saving data to a file, handling pagination, extracting data from tables, handling JavaScript-rendered content, using Scrapy for advanced scraping, avoiding getting blocked, and implementing error handling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
